{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import retro\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "from wrappers import ResizeObservation, SkipFrame, Discretizer\n",
    "from metrics import MetricLogger"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import random, numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \"\"\"mini cnn structure\n",
    "    input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "from collections import deque\n",
    "\n",
    "class Hank:\n",
    "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # Hank's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = NeuralNet(self.state_dim, self.action_dim).double()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device=\"cuda\")\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        #self.exploration_rate_decay = 0.999999\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e4  # no. of experiences between saving Hank Net\n",
    "\n",
    "        if checkpoint:\n",
    "            try:\n",
    "                self.load(checkpoint)\n",
    "            except:\n",
    "                print(f\"{checkpoint} not found! Initializing Hank anyway...\")\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "        Inputs:\n",
    "        state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
    "        Outputs:\n",
    "        action_idx (int): An integer representing which action Hank will perform\n",
    "        \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state.__array__()\n",
    "            if self.use_cuda:\n",
    "                state = torch.tensor(state).cuda()\n",
    "            else:\n",
    "                state = torch.tensor(state)\n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (LazyFrame),\n",
    "        next_state (LazyFrame),\n",
    "        action (int),\n",
    "        reward (float),\n",
    "        done(bool))\n",
    "        \"\"\"\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "\n",
    "        if self.use_cuda:\n",
    "            state = torch.tensor(state).cuda()\n",
    "            next_state = torch.tensor(next_state).cuda()\n",
    "            action = torch.tensor([action]).cuda()\n",
    "            reward = torch.tensor([reward]).cuda()\n",
    "            done = torch.tensor([done]).cuda()\n",
    "        else:\n",
    "            state = torch.tensor(state)\n",
    "            next_state = torch.tensor(next_state)\n",
    "            action = torch.tensor([action])\n",
    "            reward = torch.tensor([reward])\n",
    "            done = torch.tensor([done])\n",
    "\n",
    "        self.memory.append((state, next_state, action, reward, done,))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "                self.save_dir / f\"Hank_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"HankNet saved to {save_path} at step {self.curr_step}\")\n",
    "\n",
    "    def load(self, load_path):\n",
    "        if not load_path.exists():\n",
    "            raise ValueError(f\"{load_path} does not exist\")\n",
    "\n",
    "        ckp = torch.load(load_path, map_location=('cuda' if self.use_cuda else 'cpu'))\n",
    "        exploration_rate = ckp.get('exploration_rate')\n",
    "        state_dict = ckp.get('model')\n",
    "\n",
    "        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n",
    "        self.net.load_state_dict(state_dict)\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action).double()\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done).double()\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LAWNMOWER_LOCATION = Path().parent.absolute()\n",
    "retro.data.Integrations.add_custom_path(LAWNMOWER_LOCATION)\n",
    "\n",
    "\"\"\" CHECK NVIDIA CUDA AVAILABILITY \"\"\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\\n\")\n",
    "\n",
    "\"\"\" START ENVIRONMENT \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    save_states = [f'lawn{x}.state' for x in range(10, 0, -1)]\n",
    "    env = retro.make(game='lawnmower',\n",
    "                     state=save_states.pop(), # pops off lawn1.state\n",
    "                     inttype=retro.data.Integrations.ALL)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: lawnmower integration directory not found in the following location: {LAWNMOWER_LOCATION}\")\n",
    "    sys.exit()\n",
    "\n",
    "\"\"\" OBSERVATION WRAPPERS \"\"\"\n",
    "\n",
    "action_space = [\n",
    "    ['LEFT', 'B'],\n",
    "    ['RIGHT', 'B'],\n",
    "    ['DOWN', 'B'],\n",
    "    ['UP', 'B']\n",
    "]\n",
    "\n",
    "env = Discretizer(env, combos=action_space)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "env = TransformObservation(env, f=lambda x: x / 255.)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "\"\"\" CHECKPOINT SAVING \"\"\"\n",
    "\n",
    "save_dir = Path(\"../checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "#checkpoint = Path('..\\\\checkpoints\\\\2021-11-27T18-33-07\\\\Hank_net_18.chkpt')\n",
    "hank = Hank(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)#\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(4, 84, 84)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "init_state = env.reset()\n",
    "\n",
    "init_state.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "state = init_state\n",
    "\n",
    "state = state.__array__()\n",
    "\n",
    "state = torch.tensor(state).cuda()\n",
    "\n",
    "tensor = state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "class nn0(nn.Module):\n",
    "    def __init__(self, input_shape=(4,84,84), output_shape=4):\n",
    "        super().__init__()\n",
    "        c, h, w = input_shape\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        # playing around with size in neural\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(512, 4),\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.nn(input)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "nn0model = nn0().double().cuda()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 84, 84])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x49 and 3136x512)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mnn0model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m out\u001B[38;5;241m.\u001B[39mshape\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HankNet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36mnn0.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m---> 29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HankNet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HankNet\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HankNet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HankNet\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (64x49 and 3136x512)"
     ]
    }
   ],
   "source": [
    "out = nn0model(tensor)\n",
    "\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = NeuralNet(hank.state_dim, hank.action_dim).double().cuda()\n",
    "\n",
    "print(hank.state_dim)\n",
    "print(hank.action_dim)\n",
    "\n",
    "out = net(tensor, model = \"online\")\n",
    "\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "action = hank.act(init_state)\n",
    "prev_action = action\n",
    "action_state = init_state  # current state when action is performed\n",
    "next_state, _, _, info = env.step(action)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ram = env.get_ram()\n",
    "\n",
    "tensor = memory_to_tensor(ram)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "init_state.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tensor.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \"\"\"mini cnn structure\n",
    "    input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # playing around with size in neural\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=4, out_channels=32, kernel_size=(3,4,2), stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=32, out_channels=64, kernel_size=(2,2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=64, out_channels=64, kernel_size=(1,2,1), stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_dim),\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Old Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for e in range(episodes):\n",
    "\n",
    "    # State reset between runs\n",
    "    init_state = env.reset()\n",
    "\n",
    "    # For randomly selecting save states to work with\n",
    "    # save_state_no = np.random.randint(1,4)\n",
    "    # save_state_file = f'lawn{save_state_no}.state'\n",
    "    # env.load_state(save_state_file, inttype=retro.data.Integrations.ALL)\n",
    "\n",
    "    # Variables to keep track of for reward function\n",
    "    frame_count = 0\n",
    "    frame_since_act = 0\n",
    "    frame_since_OOF = 0\n",
    "    fuel_pickups = 0\n",
    "    turns = 0\n",
    "    propane_points = 0  # aka cumulative_reward\n",
    "\n",
    "    reward = 0\n",
    "\n",
    "    act = False\n",
    "    learn = False\n",
    "    delay_act = False\n",
    "    game_start = False\n",
    "    # new_best = False # not used\n",
    "\n",
    "    # initial action\n",
    "    action = hank.act(init_state)\n",
    "    prev_action = action\n",
    "    action_state = init_state  # current state when action is performed\n",
    "    next_state, _, _, info = env.step(action)\n",
    "    done = False\n",
    "    prev_info = info\n",
    "    frames_until_act = 3\n",
    "\n",
    "    # Episode training\n",
    "    while True:\n",
    "\n",
    "        \"\"\" FRAME SENSITIVE CONDITIONS \"\"\"\n",
    "\n",
    "        frame_count += 1\n",
    "        frame_since_act += 1\n",
    "        frames_until_act -= 1\n",
    "        # cur_fuel_pickup = 0\n",
    "        fuel_rew = 0\n",
    "\n",
    "\n",
    "\n",
    "        if not game_start and info[\"FUEL_TIME\"] < 254: # FUEL_TIME changes randomly\n",
    "            game_start = True\n",
    "            prev_action = action\n",
    "            action = hank.act(next_state)\n",
    "            act = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # equals True if action blocked, False if possible\n",
    "        act_5fr = prev_info[\"FRAME_COUNTER_5\"] == 3\n",
    "\n",
    "        if act and act_5fr:\n",
    "            delay_act = True\n",
    "\n",
    "        # Run agent on the state if action is possible\n",
    "        if ((act and not act_5fr) or delay_act) and game_start:\n",
    "            # Hank is about to act.  Learn from prior actions\n",
    "\n",
    "            hank.cache(action_state, next_state, prev_action, reward, done)\n",
    "\n",
    "            #input(f\"Learning done based on next_state = current render.  Reward = {reward}  Press any key to continue.\")\n",
    "\n",
    "\n",
    "            #print(f\"action = {prev_action}, reward = {reward}\")\n",
    "\n",
    "            # Learn\n",
    "            q, loss = hank.learn()\n",
    "            propane_points += reward\n",
    "\n",
    "            ### UNCOMMENT IF YOU WANT TO SEE INPUT BY INPUT WHAT'S GOING ON\n",
    "            #print(f\"prev_action={prev_action}, reward={reward}\")\n",
    "            #input()\n",
    "\n",
    "            # Logging\n",
    "            logger.log_step(reward, loss, q)\n",
    "\n",
    "            reward = 0\n",
    "\n",
    "            # Perform new action\n",
    "            prev_action = action\n",
    "\n",
    "\n",
    "            ### UNCOMMENT IF YOU WANT TO SEE INPUT BY INPUT WHAT'S GOING ON\n",
    "\n",
    "            action = hank.act(next_state)\n",
    "\n",
    "            #print(f\"prev_action={action}, reward={reward}\")\n",
    "\n",
    "            #print(info)\n",
    "\n",
    "            #action = int(input())\n",
    "\n",
    "            ### DEBUGGING STUFF\n",
    "            if debug is True:\n",
    "                print(frame_since_act)\n",
    "\n",
    "                ram = env.get_ram()\n",
    "                ram_tensor = memory_to_tensor(ram)\n",
    "                print_grid(ram_tensor)\n",
    "\n",
    "                dir = input(\"Mow which direction?\")\n",
    "\n",
    "                action = int(int(cardinal_input(dir)))\n",
    "                #input(\"Action made based on this state. Press any key to continue\")\n",
    "                #print(f\"next_action={action}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            action_state = next_state  # current state when action is performed\n",
    "            frame_since_act = 0\n",
    "\n",
    "            act = False  # if acted, then acting should not occur on next frame\n",
    "            delay_act = False\n",
    "\n",
    "\n",
    "\n",
    "        if debug is True:\n",
    "            print(f\"player x: {info['PLAYER_X']}\")\n",
    "            print(f\"act? {act}\")\n",
    "            print(f\"act 5 fr? {act_5fr}\")\n",
    "            print(f\"frames until act: {frames_until_act}\")\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, _, _, info = env.step(action)\n",
    "\n",
    "        ram = env.get_ram()\n",
    "        info[\"PLAYER_X\"] = ram[0x00EA]\n",
    "        info[\"PLAYER_Y\"] = ram[0x00E8] - 2\n",
    "\n",
    "        # Render frame\n",
    "        env.render()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # by default, no action on next possible frame\n",
    "        if (prev_info[\"PLAYER_X\"] != info[\"PLAYER_X\"] or\n",
    "            prev_info[\"PLAYER_Y\"] != info[\"PLAYER_Y\"] or\n",
    "                (frame_since_act > 6 and act == False)\n",
    "        ):\n",
    "            act = True\n",
    "            frames_until_act = 3\n",
    "\n",
    "        # Hacky way to handle OOF'ing\n",
    "        if info[\"FUEL\"] == 0:\n",
    "            frame_since_OOF += 1\n",
    "\n",
    "\n",
    "        \"\"\" REWARD FUNCTION INFORMATION \"\"\"\n",
    "\n",
    "        ### TODO: clean up reward section\n",
    "\n",
    "        if prev_info is not None:\n",
    "            if info[\"FUEL\"] > prev_info[\"FUEL\"]:\n",
    "                fuel_pickups += 1\n",
    "                # cur_fuel_pickup = 1\n",
    "                #fuel_rew = 1 * 100 * (1 - 1 / (1 + np.exp(-frame_count / 600)))\n",
    "                fuel_rew = 2000\n",
    "                frame_since_OOF = 0\n",
    "                #print(f\"Frame: {frame_count}, reward: {fuel_rew}\")\n",
    "            if info[\"DIRECTION\"] != prev_info[\"DIRECTION\"]:\n",
    "                turns += 1\n",
    "            else:\n",
    "                turns = 0\n",
    "            if info[\"GRASS_LEFT\"] < prev_info[\"GRASS_LEFT\"]:\n",
    "                #reward += 10\n",
    "                pass\n",
    "                #print(\"Reward Updated\")\n",
    "\n",
    "            # Penalize for OOF'ing\n",
    "            if frame_since_OOF > 3:\n",
    "                reward -= 3000\n",
    "\n",
    "        # Penalizes for turning too much\n",
    "        #reward -= (turns - 1) * turns / 1000\n",
    "\n",
    "        # reward for fuel pickup\n",
    "        reward += fuel_rew\n",
    "\n",
    "        # Penalizes for taking too long\n",
    "        #reward -= (frame_since_act + 1) / 100\n",
    "\n",
    "        \"\"\" STATE UPDATES \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # Update state\n",
    "        # state = next_state  # irrelevant now?\n",
    "\n",
    "        # Store previous info\n",
    "        prev_info = info\n",
    "\n",
    "\n",
    "\n",
    "        if debug is True:\n",
    "            if e > 0:\n",
    "                pass\n",
    "                #print(f\"Reward = {reward}\")\n",
    "                #print(f\"Turns = {turns}\")\n",
    "                #print(\"~~~current\")\n",
    "                #print(info)\n",
    "                #print(\"~~~previous\")\n",
    "                #print(prev_info)\n",
    "                #print(\"~~~\")\n",
    "                #print(\"~~~\")\n",
    "\n",
    "        \"\"\" DONE CONDITIONS \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # Check if OOF\n",
    "        if frame_since_OOF > 3 or info[\"GRASS_LEFT\"] < 1:\n",
    "            done = True\n",
    "            if info[\"GRASS_LEFT\"] < 1:\n",
    "                reward += 10000  # maybe remove this?\n",
    "\n",
    "            # Learn from final actions\n",
    "            hank.cache(action_state, next_state, prev_action, reward, done)\n",
    "\n",
    "            # Learn\n",
    "            q, loss = hank.learn()\n",
    "            propane_points += reward\n",
    "\n",
    "            # Logging\n",
    "            logger.log_step(reward, loss, q)\n",
    "\n",
    "            if propane_points < best_propane_points:\n",
    "                print(f\"Run {e} - Propane Points = {round(propane_points,1)}  ||  Top Propane Points = {round(best_propane_points,1)}\")\n",
    "            elif propane_points >= best_propane_points:\n",
    "                best_propane_points = propane_points\n",
    "                # new_best = True # not used\n",
    "                print(f\"Run {e} ~~~ NEW BEST!  Good job, Hank!  New Top Propane Points = {round(best_propane_points,1)}\")\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    \"\"\" SAVING & CHANGING LAWNS\"\"\"\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        hank.save()\n",
    "        logger.record(episode=e, epsilon=hank.exploration_rate, step=hank.curr_step)\n",
    "        if len(lawn1_clear_ep)>0:\n",
    "            print(f\"Lawn 1 cleared on episode {lawn1_clear_ep}\")\n",
    "        elif len(lawn1_clear_ep)>1:\n",
    "            print(f\"Lawn 1 cleared on episodes {lawn1_clear_ep}\")\n",
    "\n",
    "    if info[\"GRASS_LEFT\"] < 1 and save_states:\n",
    "        hank.save()\n",
    "        lawn1_clear_ep.append(e)\n",
    "        logger.record(episode=e, epsilon=hank.exploration_rate, step=hank.curr_step)\n",
    "        env.load_state(save_states.pop(), inttype = retro.data.Integrations.ALL)\n",
    "    elif not save_states:\n",
    "        sys.exit(\"HANK, YOU DID IT! YOU RAN THE GAUNTLET! LAWN 1-10 COMPLETE.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-71c9c1ac",
   "language": "python",
   "display_name": "PyCharm (HankNet)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}