{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import retro\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "from wrappers import ResizeObservation, SkipFrame, Discretizer\n",
    "from metrics import MetricLogger\n",
    "from hank import Hank\n",
    "\n",
    "from retro import RetroEnv\n",
    "\n",
    "from utils import memory_to_tensor, print_grid, cardinal_input\n",
    "\n",
    "from utils import state_to_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LAWNMOWER_LOCATION = Path().parent.absolute()\n",
    "retro.data.Integrations.add_custom_path(LAWNMOWER_LOCATION)\n",
    "\n",
    "\"\"\" CHECK NVIDIA CUDA AVAILABILITY \"\"\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\\n\")\n",
    "\n",
    "\"\"\" START ENVIRONMENT \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    save_states = [f'lawn{x}.state' for x in range(10, 0, -1)]\n",
    "    # env is a RetroEnv object https://github.com/openai/retro/blob/98fe0d328e1568836a46e5fce55a607f47a0c332/retro/retro_env.py\n",
    "\n",
    "    env = RetroEnv(game='lawnmower',\n",
    "                   state=save_states.pop(),  # pops off lawn1.state\n",
    "                   inttype=retro.data.Integrations.ALL,\n",
    "                   obs_type=retro.enums.Observations.RAM)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: lawnmower integration directory not found in the following location: {LAWNMOWER_LOCATION}\")\n",
    "    sys.exit()\n",
    "\n",
    "\"\"\" OBSERVATION WRAPPERS \"\"\"\n",
    "\n",
    "action_space = [\n",
    "    ['LEFT', 'B'],\n",
    "    ['RIGHT', 'B'],\n",
    "    ['DOWN', 'B'],\n",
    "    ['UP', 'B']\n",
    "]\n",
    "\n",
    "env = Discretizer(env, combos=action_space)\n",
    "#env = ResizeObservation(env, shape=84)\n",
    "#env = GrayScaleObservation(env, keep_dim=False)\n",
    "# env = TransformObservation(env, f=lambda x: x / 255.)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "\"\"\" CHECKPOINT SAVING \"\"\"\n",
    "\n",
    "save_dir = Path(\"../checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "#checkpoint = Path('..\\\\checkpoints\\\\2021-11-27T18-33-07\\\\Hank_net_18.chkpt')\n",
    "hank = Hank(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)#, checkpoint=checkpoint)\n",
    "\n",
    "### Set these if you want it to begin learning anew with the current nn\n",
    "#hank.exploration_rate_min = 0.1\n",
    "hank.exploration_rate = 0.1\n",
    "#hank.exploration_rate_decay = 0.9999975\n",
    "#hank.exploration_rate_decay = 0.99999975\n",
    "\n",
    "\"\"\" LOGGING \"\"\"\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "lawn1_clear_ep = []\n",
    "\n",
    "\"\"\" BEGIN TRAINING \"\"\"\n",
    "\n",
    "debug = False\n",
    "episodes = 100000\n",
    "best_propane_points = 0  # aka best_cumulative_reward\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def state_to_tensor(state):\n",
    "    frame_0 = memory_to_tensor(state[0])\n",
    "    frame_1 = memory_to_tensor(state[1])\n",
    "    frame_2 = memory_to_tensor(state[2])\n",
    "    frame_3 = memory_to_tensor(state[3])\n",
    "\n",
    "    tensor = torch.stack((frame_0, frame_1, frame_2, frame_3)).double()\n",
    "    return(tensor)\n",
    "\n",
    "init_state = env.reset()\n",
    "\n",
    "tensor = state_to_tensor(init_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 13, 32, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([1, 4, 13, 32, 7])"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4x13x32x7\n",
    "print(tensor.shape)\n",
    "\n",
    "tensor2 = tensor.unsqueeze(dim=0)\n",
    "\n",
    "tensor2.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "class nn0(nn.Module):\n",
    "    def __init__(self,input_shape = (4,13,32,7), output_shape = 4):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        super().__init__()\n",
    "        # playing around with size in neural\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=4, out_channels=32, kernel_size=(3,3,3), stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=32, out_channels=64, kernel_size=(3,3,1), stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=64, out_channels=64, kernel_size=(1,3,1), stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(1,4),\n",
    "            nn.Linear(64, 512),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(512, 4),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.nn(input)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 512])"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn0model = nn0().double()\n",
    "\n",
    "out = nn0model(tensor2)\n",
    "\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.0006,  0.0461, -0.0783, -0.0390], dtype=torch.float64,\n       grad_fn=<AddBackward0>)"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "unflatten: Provided sizes [1, 64] don't multiply up to the size of dim 0 (4) in the input tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [125]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#a = nn.Linear(1,10).double()\u001B[39;00m\n\u001B[0;32m      3\u001B[0m a \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mUnflatten(\u001B[38;5;241m0\u001B[39m,(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m64\u001B[39m))\n\u001B[1;32m----> 4\u001B[0m \u001B[43ma\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshape\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HankNet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HankNet\\lib\\site-packages\\torch\\nn\\modules\\flatten.py:138\u001B[0m, in \u001B[0;36mUnflatten.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 138\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munflattened_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HankNet\\lib\\site-packages\\torch\\_tensor.py:992\u001B[0m, in \u001B[0;36mTensor.unflatten\u001B[1;34m(self, dim, sizes)\u001B[0m\n\u001B[0;32m    990\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(sizes, OrderedDict) \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(sizes, (\u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mlist\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(sizes[\u001B[38;5;241m0\u001B[39m], (\u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mlist\u001B[39m))):\n\u001B[0;32m    991\u001B[0m     names, sizes \u001B[38;5;241m=\u001B[39m unzip_namedshape(sizes)\n\u001B[1;32m--> 992\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mTensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msizes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: unflatten: Provided sizes [1, 64] don't multiply up to the size of dim 0 (4) in the input tensor"
     ]
    }
   ],
   "source": [
    "#a = nn.Linear(1,10).double()\n",
    "\n",
    "#a = nn.Unflatten(0,(1,64))\n",
    "#a(out).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _get_conv_out(net, shape):\n",
    "    o = net.conv(torch.zeros(1, *shape).double())\n",
    "    return int(np.prod(o.size()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "action = hank.act(init_state)\n",
    "prev_action = action\n",
    "action_state = init_state  # current state when action is performed\n",
    "next_state, _, _, info = env.step(action)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "ram = env.get_ram()\n",
    "\n",
    "tensor = memory_to_tensor(ram)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(4, 10240)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_state.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([13, 32, 7])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \"\"\"mini cnn structure\n",
    "    input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # playing around with size in neural\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=4, out_channels=32, kernel_size=(3,4,2), stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=32, out_channels=64, kernel_size=(2,2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=64, out_channels=64, kernel_size=(1,2,1), stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_dim),\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Old Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for e in range(episodes):\n",
    "\n",
    "    # State reset between runs\n",
    "    init_state = env.reset()\n",
    "\n",
    "    # For randomly selecting save states to work with\n",
    "    # save_state_no = np.random.randint(1,4)\n",
    "    # save_state_file = f'lawn{save_state_no}.state'\n",
    "    # env.load_state(save_state_file, inttype=retro.data.Integrations.ALL)\n",
    "\n",
    "    # Variables to keep track of for reward function\n",
    "    frame_count = 0\n",
    "    frame_since_act = 0\n",
    "    frame_since_OOF = 0\n",
    "    fuel_pickups = 0\n",
    "    turns = 0\n",
    "    propane_points = 0  # aka cumulative_reward\n",
    "\n",
    "    reward = 0\n",
    "\n",
    "    act = False\n",
    "    learn = False\n",
    "    delay_act = False\n",
    "    game_start = False\n",
    "    # new_best = False # not used\n",
    "\n",
    "    # initial action\n",
    "    action = hank.act(init_state)\n",
    "    prev_action = action\n",
    "    action_state = init_state  # current state when action is performed\n",
    "    next_state, _, _, info = env.step(action)\n",
    "    done = False\n",
    "    prev_info = info\n",
    "    frames_until_act = 3\n",
    "\n",
    "    # Episode training\n",
    "    while True:\n",
    "\n",
    "        \"\"\" FRAME SENSITIVE CONDITIONS \"\"\"\n",
    "\n",
    "        frame_count += 1\n",
    "        frame_since_act += 1\n",
    "        frames_until_act -= 1\n",
    "        # cur_fuel_pickup = 0\n",
    "        fuel_rew = 0\n",
    "\n",
    "\n",
    "\n",
    "        if not game_start and info[\"FUEL_TIME\"] < 254: # FUEL_TIME changes randomly\n",
    "            game_start = True\n",
    "            prev_action = action\n",
    "            action = hank.act(next_state)\n",
    "            act = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # equals True if action blocked, False if possible\n",
    "        act_5fr = prev_info[\"FRAME_COUNTER_5\"] == 3\n",
    "\n",
    "        if act and act_5fr:\n",
    "            delay_act = True\n",
    "\n",
    "        # Run agent on the state if action is possible\n",
    "        if ((act and not act_5fr) or delay_act) and game_start:\n",
    "            # Hank is about to act.  Learn from prior actions\n",
    "\n",
    "            hank.cache(action_state, next_state, prev_action, reward, done)\n",
    "\n",
    "            #input(f\"Learning done based on next_state = current render.  Reward = {reward}  Press any key to continue.\")\n",
    "\n",
    "\n",
    "            #print(f\"action = {prev_action}, reward = {reward}\")\n",
    "\n",
    "            # Learn\n",
    "            q, loss = hank.learn()\n",
    "            propane_points += reward\n",
    "\n",
    "            ### UNCOMMENT IF YOU WANT TO SEE INPUT BY INPUT WHAT'S GOING ON\n",
    "            #print(f\"prev_action={prev_action}, reward={reward}\")\n",
    "            #input()\n",
    "\n",
    "            # Logging\n",
    "            logger.log_step(reward, loss, q)\n",
    "\n",
    "            reward = 0\n",
    "\n",
    "            # Perform new action\n",
    "            prev_action = action\n",
    "\n",
    "\n",
    "            ### UNCOMMENT IF YOU WANT TO SEE INPUT BY INPUT WHAT'S GOING ON\n",
    "\n",
    "            action = hank.act(next_state)\n",
    "\n",
    "            #print(f\"prev_action={action}, reward={reward}\")\n",
    "\n",
    "            #print(info)\n",
    "\n",
    "            #action = int(input())\n",
    "\n",
    "            ### DEBUGGING STUFF\n",
    "            if debug is True:\n",
    "                print(frame_since_act)\n",
    "\n",
    "                ram = env.get_ram()\n",
    "                ram_tensor = memory_to_tensor(ram)\n",
    "                print_grid(ram_tensor)\n",
    "\n",
    "                dir = input(\"Mow which direction?\")\n",
    "\n",
    "                action = int(int(cardinal_input(dir)))\n",
    "                #input(\"Action made based on this state. Press any key to continue\")\n",
    "                #print(f\"next_action={action}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            action_state = next_state  # current state when action is performed\n",
    "            frame_since_act = 0\n",
    "\n",
    "            act = False  # if acted, then acting should not occur on next frame\n",
    "            delay_act = False\n",
    "\n",
    "\n",
    "\n",
    "        if debug is True:\n",
    "            print(f\"player x: {info['PLAYER_X']}\")\n",
    "            print(f\"act? {act}\")\n",
    "            print(f\"act 5 fr? {act_5fr}\")\n",
    "            print(f\"frames until act: {frames_until_act}\")\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, _, _, info = env.step(action)\n",
    "\n",
    "        ram = env.get_ram()\n",
    "        info[\"PLAYER_X\"] = ram[0x00EA]\n",
    "        info[\"PLAYER_Y\"] = ram[0x00E8] - 2\n",
    "\n",
    "        # Render frame\n",
    "        env.render()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # by default, no action on next possible frame\n",
    "        if (prev_info[\"PLAYER_X\"] != info[\"PLAYER_X\"] or \n",
    "            prev_info[\"PLAYER_Y\"] != info[\"PLAYER_Y\"] or \n",
    "                (frame_since_act > 6 and act == False)\n",
    "        ):\n",
    "            act = True\n",
    "            frames_until_act = 3\n",
    "\n",
    "        # Hacky way to handle OOF'ing\n",
    "        if info[\"FUEL\"] == 0:\n",
    "            frame_since_OOF += 1\n",
    "\n",
    "\n",
    "        \"\"\" REWARD FUNCTION INFORMATION \"\"\"\n",
    "\n",
    "        ### TODO: clean up reward section\n",
    "\n",
    "        if prev_info is not None:\n",
    "            if info[\"FUEL\"] > prev_info[\"FUEL\"]:\n",
    "                fuel_pickups += 1\n",
    "                # cur_fuel_pickup = 1\n",
    "                #fuel_rew = 1 * 100 * (1 - 1 / (1 + np.exp(-frame_count / 600)))\n",
    "                fuel_rew = 2000\n",
    "                frame_since_OOF = 0\n",
    "                #print(f\"Frame: {frame_count}, reward: {fuel_rew}\")\n",
    "            if info[\"DIRECTION\"] != prev_info[\"DIRECTION\"]:\n",
    "                turns += 1\n",
    "            else:\n",
    "                turns = 0\n",
    "            if info[\"GRASS_LEFT\"] < prev_info[\"GRASS_LEFT\"]:\n",
    "                #reward += 10\n",
    "                pass\n",
    "                #print(\"Reward Updated\")\n",
    "\n",
    "            # Penalize for OOF'ing\n",
    "            if frame_since_OOF > 3:\n",
    "                reward -= 3000\n",
    "\n",
    "        # Penalizes for turning too much\n",
    "        #reward -= (turns - 1) * turns / 1000\n",
    "\n",
    "        # reward for fuel pickup\n",
    "        reward += fuel_rew\n",
    "\n",
    "        # Penalizes for taking too long\n",
    "        #reward -= (frame_since_act + 1) / 100\n",
    "\n",
    "        \"\"\" STATE UPDATES \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # Update state\n",
    "        # state = next_state  # irrelevant now?\n",
    "\n",
    "        # Store previous info\n",
    "        prev_info = info\n",
    "\n",
    "\n",
    "\n",
    "        if debug is True:\n",
    "            if e > 0:\n",
    "                pass\n",
    "                #print(f\"Reward = {reward}\")\n",
    "                #print(f\"Turns = {turns}\")\n",
    "                #print(\"~~~current\")\n",
    "                #print(info)\n",
    "                #print(\"~~~previous\")\n",
    "                #print(prev_info)\n",
    "                #print(\"~~~\")\n",
    "                #print(\"~~~\")\n",
    "\n",
    "        \"\"\" DONE CONDITIONS \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # Check if OOF\n",
    "        if frame_since_OOF > 3 or info[\"GRASS_LEFT\"] < 1:\n",
    "            done = True\n",
    "            if info[\"GRASS_LEFT\"] < 1:\n",
    "                reward += 10000  # maybe remove this?\n",
    "\n",
    "            # Learn from final actions\n",
    "            hank.cache(action_state, next_state, prev_action, reward, done)\n",
    "\n",
    "            # Learn\n",
    "            q, loss = hank.learn()\n",
    "            propane_points += reward\n",
    "\n",
    "            # Logging\n",
    "            logger.log_step(reward, loss, q)\n",
    "\n",
    "            if propane_points < best_propane_points:\n",
    "                print(f\"Run {e} - Propane Points = {round(propane_points,1)}  ||  Top Propane Points = {round(best_propane_points,1)}\")\n",
    "            elif propane_points >= best_propane_points:\n",
    "                best_propane_points = propane_points\n",
    "                # new_best = True # not used\n",
    "                print(f\"Run {e} ~~~ NEW BEST!  Good job, Hank!  New Top Propane Points = {round(best_propane_points,1)}\")\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    \"\"\" SAVING & CHANGING LAWNS\"\"\"\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        hank.save()\n",
    "        logger.record(episode=e, epsilon=hank.exploration_rate, step=hank.curr_step)\n",
    "        if len(lawn1_clear_ep)>0:\n",
    "            print(f\"Lawn 1 cleared on episode {lawn1_clear_ep}\")\n",
    "        elif len(lawn1_clear_ep)>1:\n",
    "            print(f\"Lawn 1 cleared on episodes {lawn1_clear_ep}\")\n",
    "\n",
    "    if info[\"GRASS_LEFT\"] < 1 and save_states:\n",
    "        hank.save()\n",
    "        lawn1_clear_ep.append(e)\n",
    "        logger.record(episode=e, epsilon=hank.exploration_rate, step=hank.curr_step)\n",
    "        env.load_state(save_states.pop(), inttype = retro.data.Integrations.ALL)\n",
    "    elif not save_states:\n",
    "        sys.exit(\"HANK, YOU DID IT! YOU RAN THE GAUNTLET! LAWN 1-10 COMPLETE.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-71c9c1ac",
   "language": "python",
   "display_name": "PyCharm (HankNet)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}